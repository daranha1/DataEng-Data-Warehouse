# Project : Data Warehouse Using Amazon Cloud

## Purpose:
   1. To build an ETL pipeline for a database hosted on Amazon Redshift.
   2. To load data from S3 to staging tables on Redshift and 
   3. Execute SQL statements that create the analytics tables from these staging tables.
   
## Current Scenario:
A startup named Sparkify has grown their user base and song database and want to move<br/>
their processes and data onto the cloud. 
* Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a 
  directory with JSON metadata on the songs in their app.
  
## Data Engineer Tasks:
1. Build an ETL pipeline that extracts Sparkify's data from S3, 
2. Stages the data in Redshift
3. Transform data into a set of dimensional tables so Sparkify's analytics team can continue <br/>
   finding insights in what songs their users are listening to. 
4. Test the database and ETL pipeline by running queries given to you by Sparkify's analytics team
5. Compare results with expected results of the analytics team.

## Project Datasets
| Data               | Location                     |
|--------------------|------------------------------|
| Song Data          | s3://udacity-dend/song_data  |
| Log Data           | s3://udacity-dend/log_data   |
| Log Data json path | s3://udacity-dend/log_json_path.json |

### Song Dataset
1. This dataset is a subset of real data from the **Million Song Dataset**
2. Each file is in JSON format and contains metadata about a song and the artist of that song.<br/>
3. The files are partitioned by the first three letters of each song's track ID.<br/>
   Filepaths for two files in this dataset are as follows:<br/>
   * **song_data/A/B/C/TRABCEI128F424C983.json**
   * **song_data/A/A/B/TRAABJL12903CDCF1A.json**
   
#### Song file Example : **TRAABJL12903CDCF1A.json**

### Log Dataset
1. The log dataset consists of log files in JSON format generated by an event simulator<br/>
   based on songs in the dataset above.<br/>
   
   **{<br/>
    "num_songs": 1, <br/>
    "artist_id": "ARJIE2Y1187B994AB7", <br/>
    "artist_latitude": null, <br/>
    "artist_longitude": null, <br/>
    "artist_location": "", <br/>
    "artist_name": "Line Renaud", <br/>
    "song_id": "SOUPIRU12A6D4FA1E1", <br/>
    "title": "Der Kleine Dompfaff", <br/>
    "duration": 152.92036, <br/>
    "year": 0 <br/>
   }**
   
#### Log file Example : log_data/2018/11/2018-12-events.json
![Example : log file : log_data/2018/11/2018-12-events.json](./images/log-data-2018-11-12-events.png)

## Cloud Software:
#### 1. S3 
* Amazon S3 or Amazon Simple Storage Service is a service offered by Amazon Web Services (AWS) <br/> that provides object storage through a web service interface.[1][2] 
* Amazon S3 uses the same scalable storage infrastructure that Amazon.com uses to run its global<br/> e-commerce network.[3] 
* Amazon S3 can be employed to store any type of object which allows for uses like storage for <br/> Internet applications, backup and recovery, disaster recovery, data archives, data lakes for <br/>
analytics, and hybrid cloud storage.<br/>

**Sources** :
1. https://en.wikipedia.org/wiki/Amazon_S3<br/>
2.  "**Amazon Web Services Launches "Amazon S3"" (Press release). 2006-03-14. <br/>
Archived from the original on 2018-11-15.** Retrieved 2018-11-14.
3. Huang, Dijiang; Wu, Huijun (2017-09-08). <br/>
**Mobile Cloud Computing: Foundations and Service Models.** <br/>
Morgan Kaufmann. p. 67. ISBN 9780128096444. <br/>
Archived from the original on 2018-11-15. Retrieved 2018-11-15.
4. "**Cloud Object Storage - Store & Retrieve Data Anywhere - Amazon Simple Storage Service**". <br/>
Amazon Web Services, Inc. Archived from the original on 2018-05-17. Retrieved 2018-05-17.
 

#### 2. Amazon RedShift
**Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud.** 
* One can start with just a few hundred gigabytes of data and scale to a petabyte or more. 
* This enables the use of data to acquire new insights for business and customers.

* To create a data warehouse a set of nodes is launched, called an Amazon Redshift cluster. 
* After the cluster is provisioned, one can upload a data set and then perform data analysis queries. 
* Regardless of the size of the data set, Amazon Redshift offers fast query performance using the <br/>
same SQL-based tools and business intelligence applications that is used today.<br/>

**Source** : https://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html

**Rationale for using Amazon Redshift**<br/>
> No other data warehouse makes it as easy to gain new insights from all your data. <br/>
With Redshift, you can query and combine exabytes of structured and semi-structured <br/>
data across your data warehouse, operational database, and data lake using standard SQL. <br/>
Redshift lets you easily save the results of your queries back to your S3 data lake using open <br/> formats, like Apache Parquet, so that you can do additional analytics from other<br/> analytics services like Amazon EMR, Amazon Athena, and Amazon SageMaker.<br/>

**Source** : https://aws.amazon.com/redshift/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc

 ## Schema for Song Play Analysis
 
| No | Table Name | Table Type | Column              | diststyle |
|----|------------|------------|---------------------|-----------|
| 1  | songplays  | Fact       | songplay_id SORTKEY |           |
|    |            |            | artist_id  DISTKEY  |           |
| 2  | artists    | Dimension  | artist_id SORTKEY   | ALL       |
| 3  | songs      | Dimension  | song_id SORTKEY     |           |
| 4  | users      | Dimension  | user_id SORTKEY     | ALL       |
|    |            |            |                     |           |
| 5  | time       | Dimension  | start_time SORTKEY  |           |
* Small tables (e.g. lookup tables like artists and users can be replicated on all slices to <br/>
speed up joins
* Distibuting a dimension table with distyle ALL eliminates shuffling
* Using a SORTKEY means rows are sorted before distribution to slices.  This will minimize <br/>
the query time since each node has contiguous ranges of rows based on the sorting key.<br/>

![Schema Design](./images/sparkify-schema-data-warehouse.png)

### Implementation
#### A. Files (Script files, configuration file, Jupyter notebook file)
| No | Files                | Purpose                                                               |
|----|--------------------- |-----------------------------------------------------------------------|
| 1  | **sql_queries.py**   | It contains queries that are imported by the create tables script |
|    |                      | and by the extract transform load script -- etl.py: |
|    |                      | 1.1. To create staging tables, and  Fact and Dimension tables |
|    |                      | 1.2  To drop tables if they exist -- staging,fact,dimension tables |
|    |                      | 1.3  To copy S3 datasets to staging tables in etl.py |
| 2  | **create_tables.py** | 2.1. It connects to the database and imports code |
|    |                      | 2.2. It imports code from sql_queries.py to create tables|
| 3  | **etl.py**           | 3.1. It loads data from S3 to staging tables on Redshift |
|    |                      | 3.2  It loads data from staging tables into fact and dimension tables |
| 4  | **dwh.cfg**          | This is a configuration file with parameters for : |
|    |                      | 4.1 AWS Key and Secret |
|    |                      | 4.2 AWS Redshift Cluster |
|    |                      | 4.3 AWS Data Warehouse Info |
|    |                      | 4.4 IAM_Role which provides permission to copy the datasets in S3 to |
|    |                      | staging tables in Redshift |
|    |                      | 4.5 The raw datasets in S3 which are part of the copy_queries that are |
|    |                      | loaded into the staging tables in Redshift in the script etl.py |
| 5  | **Redshift_Cluster_Setup_Without_Output** | A Jupyter notebook file - discussed below in |
|    |                                           | section **B.2**  Any user can run this file with | 
|    |                                           | their specific data.  |
|    |                                           | Output was removed from this file for security | 
|    |                                           | reasons |
| 6  | **Redshift_Cluster_Setup_With_Output.html | The executed notebook file.  |
|    |                                           | Account related data has been replaced with 12 X's|                                                                                                      #### B. Configuration and Redshift Cluster Setup
#### B.1. Configuration (dwh.cfg)
         
| Section | Parameter | Value |
|------------|-----------|-------|
| **AWS**    | Key       | your key |
|            | SECRET    | your secret |
| **CLUSTER** | HOST      | dwhcluster.XXXXXXXXXXXX.us-west-2.redshift.amazonaws.com |
|         | DB_NAME   | dwh |
|         | DB_USER   | dwhuser |
|         | DB_PASSWORD | PasswordXXX | 
|         | DB_PORT     | 5439 |
| **DWH** | DWH_CLUSTER_TYPE | multi-node |
|         | DWH_NUM_NODES | 4 |
|         | DWH_NODE_TYPE | dc2.large |
|         | DWH_IAM_ROLE  | myRedshiftRole |
|         | DWH_CLUSTER_IDENTIFIER | dwhCluster |
|         | DWH_DB | dwh |
|         | DWH_DB_USER | dwhuser |
|         | DWH_DB_PASSWORD | PasswordXXX |
|         | DWH_PORT | 5439 |
| **IAM_ROLE** | ARN | arn:aws:iam::XXXXXXXXXXXX:role/my/RedshiftRole |
| **S3** | LOG_DATA | s3://udacity-dend/log_data |
|        | LOG_JSONPATH | s3://udacity-dend/log_json_path.json |
|        | SONG_DATA | s3://udacity-dend/song_data |

#### B.2 Redshift Cluster Setup /Delete And Running Python Scripts 
* Create a Data warehouse user with Administrator access and an IAM role that has read access to S3
* Launch Redshift cluster programatically using infrastructure as code

**Steps Involved in programmatic Redshift Cluster Setup in Jupyter notebook file**
1. Step 0 : Ensure that config file has the correct values for the Key and Secret parameters
2. Load DWH parameters from the config file -- dwh.cfg
3. Create clients for IAM, EC2, S3 and Redshift using the Amazon service name,<br/>
   the region name, us-west-2 (Oregon), and the aws access key and the aws secret key
4. Check the sample data sources on S3
5. Step 1 : Create an IAM Role that allows Redshift to access the S# bucket with read only  permission
6. Step 2 : Create a Redshift Cluster with parameters from the config file
7. Step 2.1 : Check the Cluster details and status
8. Step 2.2 : When the cluster status=available, find the cluster endpoint and role ARN
9. Step 3 : Open an incoming TCP port to access the cluster endpoint
   with GroupName='default'
10. Step 4: Ensure that a connection is made to the Redshift cluster using the database
    parameters provided in the config file.<br/>
    More specifically, we connect to postgresql with database details : username, password, the<br/> endpoint, the port and the database name
11. Step 5 : Run Python script to create tables.
    This will create staging tables and fact and dimensional tables in Redshift
12. Step 6 : Run Python script etl.py to execute the Extract, Transform, Load Processes.<br/>
    Copy from S3 the raw data to load the Staging Tables in Redshift<br/>
    Transform Step : Insert data from staging tables into the fact and dimension tables<br/>
13. Step 7 : Clean up resources :
    * Delete IAM_Role
    * Delete Redshift Cluster


#### C.  Build ETL Pipeline
* Code for extract, transform, load resides in etl.py discussed above.  See **A.Files** in Implementation

## D. Python and Notebook Environment
1. **Running in Udacity Workspace**<br/>
   Python 3, python packages, jupyter notebooks, and terminal/console  are available<br/>
2. **In the Jupyter notebook** import pandas, boto3 and json.<br/>
   Boto3 is the Amazon Web Services Software Development Kit for Python, which allows<br/> Python developers to write software that makes use of services like Amazon S3 and <br/>
   amazon EC2<br/>.
3. **In sql_queries.py** import configparser to process parameters for the staging tables<br/>
   and for the IAM_Role.<br/>
4. **In create_tables.py** : <br/>
   import configparser to process parameters from the config file<br/>
   import psycopg2 - a python library - to connect to the database<br/>
   import create and drop table queries from sql_queries.py<br/>
5. **In etl.py** : <br/>
   import configparser to process parameters from the config file<br/>
   import psycopg2 - a python library - to connect to the database<br/>
   import copy and insert table queries from sql_queries.py<br/>
   
 **Note** : create_tables.py and etl.py are executed in the Jupyter notebook

## E. Running Queries in Redshift Query Editor
##### E.1 Checking Row Counts of Tables in Redshift
1. SELECT COUNT(*) AS staging_events_row_count<br/>
  FROM staging_events;<br/>
**staging_events_row_count: 8056**<br/>

2. SELECT COUNT(*) AS staging_songs_row_count <br/>
   FROM staging_songs;<br/>
**staging_songs_row_count: 14896**<br/>

3. SELECT COUNT(*) AS songplays_row_count<br/>
   FROM songplays;<br/>
**songplays_row_count: 9957**<br/>

4. SELECT COUNT(*) AS users_row_count<br/>
FROM users;<br/>
**users_row_count : 104**<br/>

5. SELECT COUNT(*) AS songs_row_count<br/>
FROM songs;<br/>
**songs_row_count : 14896**<br/>

6. SELECT COUNT(*) AS artists_row_count<br/>
FROM artists;<br/>
**artist_row_count : 10025**<br/>

7. select count(*) AS time_row_count<br/>
FROM time;<br/>
**time_row_count : 6813**<br/>

##### E.2 Analytic Queries
1. SELECT level, count(level) from users group by level;

| Level | Count |
|-------|-------|
| Free  | 82 |
| Paid  | 22 |

![Query Stats](./images/query-stats-users-level.png)<br/><br/>

![Query Details](./images/query-details-users-group-by-level.png)

2. SELECT a.name AS artist_name, Count(*) AS No_of_plays 
FROM songplays AS sp <br/>
INNER JOIN artists AS a <br/>
ON sp.artist_id = a.artist_id<br/> 
GROUP BY artist_name <br/>
ORDER BY No_of_Plays DESC<br/>
LIMIT 8;<br/>

| Artist_Name | No_of_plays |
|------------------|--------|
| Muse             |   245  |
| Radiohead	       |   240  |
| Coldplay	       |   232  |
| Kings Of Leon	   |   220  |
| Alliance Ethnik  |   150  |
| Foo Fighters	   |   126  |
| The Black Keys   |   108  |
| The Beastie Boys |   105  |

3. SELECT so.title AS song_title, Count(*) AS No_of_Plays <br/>
FROM songplays AS sp <br/>
INNER JOIN songs AS so <br/>
ON sp.song_id = so.song_id <br/>
GROUP BY title<br/>
ORDER BY No_of_Plays DESC <br/>
LIMIT 8;<br/>

| song_title | no_of_plays |
|------------------------------------------------|-----|
| Speed Of Sound (Live)	                         | 58  |
| A Rush Of Blood To The Head (Live In Sydney)	 | 58  |
| Don't Panic	                                 | 58  |
| One I Love	                                 | 58  |
| Ragoo	                                         | 55  |
| Day Old Blues	                                 | 55  |
| Genius	                                     | 55  |
| Wicker Chair	                                 | 55  |

### References
1. https://popsql.com/learn-sql/redshift/how-to-use-distkey-sortkey-and-define-column-compression-encoding-in-redshift<br/>
How to Use DISTKEY, SORTKEY and Define Column Compression Encoding in Redshift<br/>
2. https://aws.amazon.com/blogs/big-data/amazon-redshift-engineerings-advanced-table-design-playbook-distribution-styles-and-distribution-keys/<br/>
AWS Big Data Blog<br/>
Amazon Redshift Engineering's Advanced Table Design Playbook:<br/>
Distribution Styles and Distribution Keys

